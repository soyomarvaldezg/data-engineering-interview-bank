# PySpark en Entrevistas de Data Engineering

## ¿Por qué PySpark?

PySpark es el **framework más usado** en data engineering moderno. Esperan que entiendas:

- **RDD vs DataFrame**: Cuándo usar cada uno
- **Transformations & Actions**: Lazy evaluation
- **Optimization & Caching**: Performance tuning
- **Partitioning & Shuffling**: Escalabilidad
- **Window Functions & Aggregations**: Análisis complejos

## Temas cubiertos

1. **RDD vs DataFrame** — Diferencias, ventajas/desventajas
2. **Transformations & Actions** — map, filter, reduce, collect, etc.
3. **Optimization & Caching** — persistence, broadcasting, bucketing
4. **Performance Tuning** — parallelism, partitioning, shuffle optimization
5. **Window Functions en Spark** — similar a SQL pero en código
6. **Data Quality en Spark** — NULL handling, duplicates, validation

## Recomendación de estudio

Empieza con **RDD vs DataFrame** → luego **Transformations** → después **Optimization**.

Después que entiendas estos, **PySpark SQL** te parecerá más fácil.
